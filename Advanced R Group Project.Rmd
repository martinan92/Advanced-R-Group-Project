---
title: "Advanced R Group Project"
author: 
date: "6/3/2019"
output: html_document
---

```{r setup}
source('./load_libraries.R')

raw_bank_train <- fread("Data/BankCamp_train.csv")
raw_bank_test <- fread("Data/BankCamp_test.csv")

```

## SMOTE
```{r}
fixed_train_df <- fix_types(create_weekday(raw_bank_train))

train_test <- f_partition(fixed_train_df, test_proportion=0.1, seed=20190603)

train_to_smote <- train_test$train
holdout <- train_test$test

smote_train_df <- SMOTE(y ~ ., train_to_smote, perc.over = 500)
print(summary(smote_train_df))

fwrite(smote_train_df, file = "Data/smote_train.csv")
```


## Baselines
### Logit
```{r}

pp <- c(scale_df)

logit_baseline_smote <- pipeline_casero(smote_train_df, preprocessing=pp, model="glm")
saveRDS(logit_baseline_smote, "./saved_models/logit_baseline_smote.rds")

print(logit_baseline_smote$results[c("AUC", "Sensitivity", "Precision", "Accuracy")])

logit_holdout <- scale_df(holdout)

preds <- predict(logit_baseline_smote, logit_holdout)

sensitivity(data=preds, reference=factor(logit_holdout$y, levels=c("yes", "no")),positive="yes")
specificity(data=preds, reference=factor(logit_holdout$y, levels=c("yes", "no")),positive="yes")
precision(data=preds, reference=factor(logit_holdout$y, levels=c("yes", "no")),positive="yes")
accuracy(data=data.frame(pred=preds, obs=logit_holdout$y))

table(preds, holdout$y)
```


### Random Forest
```{r}
tg <- data.table(expand.grid(mtry=15,
                             splitrule='gini',
                             min.node.size=5))

rf_baseline_smote <- pipeline_casero(smote_train_df, tunegrid=tg)
saveRDS(rf_baseline_smote, "./saved_models/rf_baseline_smote.rds")

print(rf_baseline_smote$results[c("AUC", "Sensitivity", "Precision", "Accuracy")])

preds <- predict(rf_baseline_smote, holdout)

sensitivity(data=preds, reference=factor(holdout$y, levels=c("yes", "no")),positive="yes")
specificity(data=preds, reference=factor(holdout$y, levels=c("yes", "no")),positive="yes")
precision(data=preds, reference=factor(holdout$y, levels=c("yes", "no")),positive="yes")
accuracy(data=data.frame(pred=preds, obs=holdout$y))

table(preds, holdout$y)

```


### XG Boost
```{r}
# So many tuning parameters wow
tg <- data.table(expand.grid(eta=0.4,
                             max_depth='2',
                             nrounds=500,
                             gamma=10,
                             colsample_bytree=0.5,
                             min_child_weight=5,
                             subsample=0.9))

xg_baseline_smote <- pipeline_casero(smote_train_df, model="xgbTree", tunegrid=tg)

print(xg_baseline_smote$results[c("AUC", "Sensitivity", "Precision", "Accuracy")])

preds <- predict(xg_baseline_smote, holdout)

sensitivity(data=preds, reference=factor(holdout$y, levels=c("yes", "no")),positive="yes")
specificity(data=preds, reference=factor(holdout$y, levels=c("yes", "no")),positive="yes")
precision(data=preds, reference=factor(holdout$y, levels=c("yes", "no")),positive="yes")
accuracy(data=data.frame(pred=preds, obs=holdout$y))

table(preds, holdout$y)
saveRDS(xg_baseline_smote, "./saved_models/xg_baseline_smote.rds")

```

## Hyperparameter Tuning

### Random Forest

```{r}
tg_RF <- data.table(expand.grid(mtry = 2:4,
                                splitrule=c("gini", "extratrees"),
                                min.node.size = c(1, 5, 10)))
                    
rf_tuned <- pipeline_casero(smote_train_df, tunegrid=tg_RF)

saveRDS(rf_tuned, "rf_tuned.rds")
```


### XG Boost

```{r}
# Tuning Round 1
nrounds <- 500
tg_XG1 <- expand.grid(nrounds = seq(from = 200, to = nrounds, by = 50),
                               eta = c(0.025, 0.05, 0.1, 0.3),
                               max_depth = c(2, 3, 4, 5, 6),
                               gamma = 0,
                               colsample_bytree = 1,
                               min_child_weight = 1,
                               subsample = 1)

xg_tuned1 <- pipeline_casero(smote_train_df, model="xgbTree", tunegrid=tg_XG1)

# Tuning Round 2
tg_XG2 <- expand.grid(nrounds = seq(from = 50, to = nrounds, by = 50),
                               eta = xg_tuned1$bestTune$eta,
                               max_depth = ifelse(xg_tuned1$bestTune$max_depth == 2,
                                                  c(xg_tuned1$bestTune$max_depth:4),
                                                  xg_tuned1$bestTune$max_depth - 1:xg_tuned1$bestTune$max_depth + 1),
                               gamma = 0,
                               colsample_bytree = 1,
                               min_child_weight = c(1, 2, 3),
                               subsample = 1)

xg_tuned2 <- pipeline_casero(smote_train_df, model="xgbTree", tunegrid=tg_XG2)

# Tuning Round 3
tg_XG3 <- expand.grid(nrounds = seq(from = 50, to = nrounds, by = 50),
                               eta = xg_tuned1$bestTune$eta,
                               max_depth = xg_tuned2$bestTune$max_depth,
                               gamma = 0,
                               colsample_bytree = c(0.4, 0.6, 0.8, 1.0),
                               min_child_weight = xg_tuned2$bestTune$min_child_weight,
                               subsample = c(0.5, 0.75, 1.0))
                   
xg_tuned3 <- pipeline_casero(smote_train_df, model="xgbTree", tunegrid=tg_XG3)

# Tuning Round 4
tg_XG4 <- expand.grid(nrounds = seq(from = 50, to = nrounds, by = 50),
                                eta = xg_tuned1$bestTune$eta,
                                max_depth = xg_tuned2$bestTune$max_depth,
                                gamma = c(0, 0.05, 0.1, 0.5, 0.7, 0.9, 1.0),
                                colsample_bytree = xg_tuned3$bestTune$colsample_bytree,
                                min_child_weight = xg_tuned2$bestTune$min_child_weight,
                                subsample = xg_tuned3$bestTune$subsample)

xg_tuned4 <- pipeline_casero(smote_train_df, model="xgbTree", tunegrid=tg_XG4)

# Tuning Round 5
tg_XG5 <- expand.grid(nrounds = seq(from = 100, to = 10000, by = 100),
                                eta = c(0.01, 0.015, 0.025, 0.05, 0.1),
                                max_depth = xg_tuned2$bestTune$max_depth,
                                gamma = xg_tuned4$bestTune$gamma,
                                colsample_bytree = xg_tuned3$bestTune$colsample_bytree,
                                min_child_weight = xg_tuned2$bestTune$min_child_weight,
                                subsample = xg_tuned3$bestTune$subsample)

xg_tuned5 <- pipeline_casero(smote_train_df, model="xgbTree", tunegrid=tg_XG5)

# Final Tuning Round
final_grid <- expand.grid(nrounds = xg_tuned5$bestTune$nrounds,
                                    eta = xg_tuned5$bestTune$eta,
                                    max_depth = xg_tuned5$bestTune$max_depth,
                                    gamma = xg_tuned5$bestTune$gamma,
                                    colsample_bytree = xg_tuned5$bestTune$colsample_bytree,
                                    min_child_weight = xg_tuned5$bestTune$min_child_weight,
                                    subsample = xg_tuned5$bestTune$subsample)

xg_final <- pipeline_casero(smote_train_df, model="xgbTree", tunegrid=final_grid)

saveRDS(xg_final, "xg_final.rds")
                                  
```

