---
title: "Advanced R Group Project"
author: 
date: "6/3/2019"
output: html_document
---

```{r setup}
# source('Code/load_libraries.R')
source('Code/data_cleaning.R')

raw_bank_train <- fread("Data/BankCamp_train.csv")
raw_bank_test <- fread("Data/BankCamp_test.csv")

```

## SMOTE
```{r}
fixed_train_df <- fix_types(raw_bank_train)
fixed_test_df <- fix_types(raw_bank_test)

fixed_train_df$y <- ifelse(fixed_train_df$y=='no',0,1)
fixed_train_df$y <- as.factor(fixed_train_df$y)

smote_train_df <- SMOTE(y ~ ., fixed_train_df, perc.over = 500)
summary(smote_train_df)

write.csv(smote_train_df, file = "Data/smote_train.csv")
```


## Baseline
### Logit
```{r}
logit_baseline <- pipeline_casero(smote_train_df, model="glm")
saveRDS(logit_baseline, "./saved_models/logit_baseline.rds")

```


### Random Forest
```{r}
tg <- data.table(expand.grid(mtry=15,
                             splitrule='gini',
                             min.node.size=5))

rf_baseline <- pipeline_casero(smote_train_df, tunegrid=tg)
saveRDS(rf_baseline, "./saved_models/rf_baseline.rds")

```


### XG Boost
```{r}
xg_baseline <- pipeline_casero(smote_train_df, model="xgbTree")
saveRDS(xg_baseline, "./saved_models/xg_baseline.rds")

```

## Hyperparameter Tuning

### Random Forest

```{r}
tg_RF <- data.table(expand.grid(mtry = 2:4,
                                splitrule=c("gini", "extratrees"),
                                min.node.size = c(1, 5, 10)))
                    
rf_tuned <- pipeline_casero(smote_train_df, tunegrid=tg_RF)

saveRDS(rf_tuned, "rf_tuned.rds")
```


### XG Boost

```{r}
# Tuning Round 1
nrounds <- 500
tg_XG1 <- expand.grid(nrounds = seq(from = 200, to = nrounds, by = 50),
                               eta = c(0.025, 0.05, 0.1, 0.3),
                               max_depth = c(2, 3, 4, 5, 6),
                               gamma = 0,
                               colsample_bytree = 1,
                               min_child_weight = 1,
                               subsample = 1)

xg_tuned1 <- pipeline_casero(smote_train_df, model="xgbTree", tunegrid=tg_XG1)

# Tuning Round 2
tg_XG2 <- expand.grid(nrounds = seq(from = 50, to = nrounds, by = 50),
                               eta = xg_tuned1$bestTune$eta,
                               max_depth = ifelse(xg_tuned1$bestTune$max_depth == 2,
                                                  c(xg_tuned1$bestTune$max_depth:4),
                                                  xg_tuned1$bestTune$max_depth - 1:xg_tuned1$bestTune$max_depth + 1),
                               gamma = 0,
                               colsample_bytree = 1,
                               min_child_weight = c(1, 2, 3),
                               subsample = 1)

xg_tuned2 <- pipeline_casero(smote_train_df, model="xgbTree", tunegrid=tg_XG2)

# Tuning Round 3
tg_XG3 <- expand.grid(nrounds = seq(from = 50, to = nrounds, by = 50),
                               eta = xg_tuned1$bestTune$eta,
                               max_depth = xg_tuned2$bestTune$max_depth,
                               gamma = 0,
                               colsample_bytree = c(0.4, 0.6, 0.8, 1.0),
                               min_child_weight = xg_tuned2$bestTune$min_child_weight,
                               subsample = c(0.5, 0.75, 1.0))
                   
xg_tuned3 <- pipeline_casero(smote_train_df, model="xgbTree", tunegrid=tg_XG3)

# Tuning Round 4
tg_XG4 <- expand.grid(nrounds = seq(from = 50, to = nrounds, by = 50),
                                eta = xg_tuned1$bestTune$eta,
                                max_depth = xg_tuned2$bestTune$max_depth,
                                gamma = c(0, 0.05, 0.1, 0.5, 0.7, 0.9, 1.0),
                                colsample_bytree = xg_tuned3$bestTune$colsample_bytree,
                                min_child_weight = xg_tuned2$bestTune$min_child_weight,
                                subsample = xg_tuned3$bestTune$subsample)

xg_tuned4 <- pipeline_casero(smote_train_df, model="xgbTree", tunegrid=tg_XG4)

# Tuning Round 5
tg_XG5 <- expand.grid(nrounds = seq(from = 100, to = 10000, by = 100),
                                eta = c(0.01, 0.015, 0.025, 0.05, 0.1),
                                max_depth = xg_tuned2$bestTune$max_depth,
                                gamma = xg_tuned4$bestTune$gamma,
                                colsample_bytree = xg_tuned3$bestTune$colsample_bytree,
                                min_child_weight = xg_tuned2$bestTune$min_child_weight,
                                subsample = xg_tuned3$bestTune$subsample)

xg_tuned5 <- pipeline_casero(smote_train_df, model="xgbTree", tunegrid=tg_XG5)

# Final Tuning Round
final_grid <- expand.grid(nrounds = xg_tuned5$bestTune$nrounds,
                                    eta = xg_tuned5$bestTune$eta,
                                    max_depth = xg_tuned5$bestTune$max_depth,
                                    gamma = xg_tuned5$bestTune$gamma,
                                    colsample_bytree = xg_tuned5$bestTune$colsample_bytree,
                                    min_child_weight = xg_tuned5$bestTune$min_child_weight,
                                    subsample = xg_tuned5$bestTune$subsample)

xg_final <- pipeline_casero(smote_train_df, model="xgbTree", tunegrid=final_grid)

saveRDS(xg_final, "xg_final.rds")
                                  
```

